## **Sequence/Sentence-oriented Operation**


### MASS
  * [paper](https://arxiv.org/pdf/1905.02450.pdf): Kaitao S. , Xu T. , Tao Q. , Jianfeng L. , Tie-Y. L. . (2019). *MASS: Masked Sequence to Sequence Pre-training for Language Generation*.
  
### UniLM
  * [paper](https://arxiv.org/abs/1905.03197): Dong, L. , Yang, N. , Wang, W. , Wei, F. , Liu, X. , & Wang, Y. , et al. (2019). *Unified language model pre-training for natural language understanding and generation*. NeurIPS 2019.
  * code: https://github.com/microsoft/unilm
  * note: including UniLM v1/v2, MiniLM, LayoutLM, and s2s-ft.
  
### BART
  * [paper](https://arxiv.org/pdf/1910.13461.pdf): Lewis, M. , Liu, Y. , Goyal, N. , Ghazvininejad M. , Mohamed A. , Levy O. , Stoyanov, V. , & Zettlemoyer, L. . (2019). *BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension*.

### seq2seq
  * link: https://github.com/google/seq2seq
  * author: google
  * note: a general-purpose encoder-decoder framework for tensorflow.
  
### pytorch-seq2seq
  * https://github.com/IBM/pytorch-seq2seq
  * author: IBM
  * note: an open source framework for seq2seq models in pytorch. 

### pytorch-seq2seq
  * link: https://github.com/bentrevett/pytorch-seq2seq
  * author: Ben Trevett
  * note: tutorials on implementing a few sequence-to-sequence (seq2seq) models with pytorch and torchtext.

### tensorflow-seq2seq-tutorials
  * link: https://github.com/ematvey/tensorflow-seq2seq-tutorials
  * author: Matvey Ezhov
  * note: dynamic seq2seq in tensorflow, step by step

### bert_seq2seq
  * link: https://github.com/920232796/bert_seq2seq
  * author: zhaohu xing
  * note: a slight framework about unilm(bert)-based implementation with several examples by pytorch.
  


