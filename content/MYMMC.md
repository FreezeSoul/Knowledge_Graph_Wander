## **Make Your Models More Clever**

---
## Knowledge into language model

### Papers
  * Dai, D., Dong, L., Hao, Y., Sui, Z., & Wei, F. (2021). [Knowledge neurons in pretrained transformers](https://arxiv.org/abs/2104.08696). arXiv preprint arXiv:2104.08696.
  * Li, L., Xu, C., Wu, W., Zhao, Y., Zhao, X., & Tao, C. (2020). [Zero-resource knowledge-grounded dialogue generation](https://arxiv.org/abs/2008.12918). arXiv preprint arXiv:2008.12918.
  * Zhou, W., Lee, D. H., Selvam, R. K., Lee, S., Lin, B. Y., & Ren, X. (2020). [Pre-training text-to-text transformers for concept-centric common sense](https://arxiv.org/abs/2011.07956). arXiv preprint arXiv:2011.07956. 
  * Heinzerling, B., & Inui, K. (2020). [Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries](https://arxiv.org/abs/2008.09036). arXiv preprint arXiv:2008.09036. 
  * Wang, C., Liu, X., & Song, D. (2020). [Language models are open knowledge graphs](https://arxiv.org/abs/2010.11967). arXiv preprint arXiv:2010.11967.
  * He, B., Jiang, X., Xiao, J., & Liu, Q. (2020). [KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning](https://arxiv.org/abs/2012.03551). arXiv preprint arXiv:2012.03551.
  * Cui, L., Cheng, S., Wu, Y., & Zhang, Y. (2020). [Does BERT Solve Commonsense Task via Commonsense Knowledge?](https://arxiv.org/abs/2008.03945). arXiv preprint arXiv:2008.03945.
  * Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., & Riedel, S. (2019). [Language models as knowledge bases?](https://arxiv.org/abs/1909.01066). arXiv preprint arXiv:1909.01066.
  * Wang, X., Gao, T., Zhu, Z., Zhang, Z., Liu, Z., Li, J., & Tang, J. (2021). [KEPLER: A unified model for knowledge embedding and pre-trained language representation](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00360/98089/KEPLER-A-Unified-Model-for-Knowledge-Embedding-and). Transactions of the Association for Computational Linguistics, 9, 176-194.
  * Talmor, A., Herzig, J., Lourie, N., & Berant, J. (2018). [Commonsenseqa: A question answering challenge targeting commonsense knowledge](https://arxiv.org/abs/1811.00937). arXiv preprint arXiv:1811.00937.
  * Bosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., & Choi, Y. (2019). [Comet: Commonsense transformers for automatic knowledge graph construction](https://arxiv.org/abs/1906.05317). arXiv preprint arXiv:1906.05317.
  * Wang, C., & Jiang, H. (2018). [Explicit utilization of general knowledge in machine reading comprehension](https://arxiv.org/abs/1809.03449). arXiv preprint arXiv:1809.03449.
  * Sap, M., Le Bras, R., Allaway, E., Bhagavatula, C., Lourie, N., Rashkin, H., ... & Choi, Y. (2019, July). [Atomic: An atlas of machine commonsense for if-then reasoning](https://ojs.aaai.org/index.php/AAAI/article/view/4160). In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, No. 01, pp. 3027-3035).


---
## Prompt for language model

### PromptPapers
  * link: https://github.com/thunlp/PromptPapers
  * author: THUNLP
  * note: must-read papers on prompt-based tuning for pre-trained language models.

### Instruction Tuning
  * [paper](https://arxiv.org/pdf/2109.01652v1.pdf): FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS.
  * blog:
    - [Instruction Tuning｜谷歌Quoc V.Le团队提出精调新范式！香过Prompt！](https://mp.weixin.qq.com/s/DcdYRzdGkF5MaL5M3yGLTw)
    - [别再Prompt了！谷歌提出tuning新方法，强力释放GPT-3潜力！](https://mp.weixin.qq.com/s/TLdKRvG1Hdsjak8AeNG-3w)
