## **Tricks to Sharpen your Models**

### :star2: Competitions
  * ALL
    - [AI-Competition-Collections](https://github.com/SWHL/AI-Competition-Collections): 收集整理各种人工智能比赛经验帖。
  * NLP
    - [NLPer-Arsenal](https://github.com/TingFree/NLPer-Arsenal): 收录NLP竞赛策略实现、各任务baseline、相关竞赛经验贴（当前赛事、往期赛事、训练赛）、NLP会议时间、常用自媒体、GPU推荐等，持续更新中。
    - [NLP Competitions List Review](https://github.com/zhpmatrix/nlp-competitions-list-review): 复盘所有NLP比赛的TOP方案，只关注NLP比赛，持续更新中！
    - [China AI & Law Challenge (CAIL)](https://github.com/china-ai-law-challenge): 历届法研杯竞赛信息。
    - [MRC_Competition_Dureader](https://github.com/luhua-rain/MRC_Competition_Dureader): 机器阅读理解 冠军/亚军代码及中文预训练MRC模型。
  * Data Science
    - [Competition Baseline](https://github.com/datawhalechina/competition-baseline): 数据科学竞赛知识、代码、思路。
    - [Data Competition TopSolution](https://github.com/Smilexuhc/Data-Competition-TopSolution): 数据竞赛Top解决方案开源整理。
    - [CDCS](https://github.com/geekinglcq/CDCS): Chinese Data Competitions' Solutions.
    - [Data-Science-Competitions](https://github.com/the-black-knight-01/Data-Science-Competitions): Goal of this repo is to provide the solutions of all Data Science Competitions(Kaggle, Data Hack, Machine Hack, Driven Data etc...).

---

### Adversarial Training
  * Survey
    - blog: [一文搞懂NLP中的对抗训练FGSM/FGM/PGD/FreeAT/YOPO/FreeLB/SMART](https://zhuanlan.zhihu.com/p/103593948)
  * FGM(Fast Gradient Method)
  * PGD(Projected Gradient Descent)
  * FreeLB (Free Large-Batch)

### Attention
  * Survey
    - blog: [各种各样神奇的自注意力机制（Self-attention）变形](https://mp.weixin.qq.com/s/HRRS5Piy_SsZy41WYg-H8Q)
  * DIN & DIEN
    - blog: [DIN+DIEN，机器学习唯一指定涨点技Attention](https://mp.weixin.qq.com/s/oRoy82I_8S7uvMToMouIeQ)
  
### Batch Size
  * Size
    - blog: [一番实验后，有关Batch Size的玄学被打破了](https://mp.weixin.qq.com/s/eMxxpPwRYSeYy9suUSX_gQ)

### BlockShuffle

### Concatenation of Embeddings
  * Automated Concatenation of Embeddings
    - [code](https://github.com/Alibaba-NLP/ACE#download-embeddings)
    - [paper](https://arxiv.org/abs/2010.05006)

### Cross-Validation
  * kFCV(k-Fold Cross-Validation)
  * LOO(Leave-One-Out Cross-Validation)

### Data Augmentation
  * Survey
    - blog: [一篇就够！数据增强方法综述](https://mp.weixin.qq.com/s/HPItY9xXJcOZWisBGOrkSw)
    - blog: [KDD 2022 | 如何正使用数据增强提高模型鲁棒性？](https://mp.weixin.qq.com/s/FKo3xF8JJ90PuobMs-mevQ)
  * SDE
  * EDA
  * FlipDA: [为大模型定制的数据增强方法FlipDA，屠榜六大NLU 数据集！](https://mp.weixin.qq.com/s/EcC8naSuNrTNQf0Es32YHQ)

### Distillation
  * Survey
    - blog: 
      - [BERT蒸馏完全指南｜原理/技巧/代码](https://mp.weixin.qq.com/s/tKfHq49heakvjM0EVQPgHw)
      - [深度神经网络模型蒸馏Distillation](https://zhuanlan.zhihu.com/p/71986772)
      - [简单实现知识蒸馏 (Knowledge Distill)](https://mp.weixin.qq.com/s/vFuCj5JoXOV3WTU_eQpClA)
    - [awesome-knowledge-distillation](https://github.com/dkozlov/awesome-knowledge-distillation)
  * Dataset Distillation
    - [一个项目帮你了解数据集蒸馏Dataset Distillation](https://mp.weixin.qq.com/s/tSKQoOLJhmdtAjxvH6z8cw) 

### Distribution / Parallel
  * Survey
    - blog:
      - [新生手册：PyTorch分布式训练](https://mp.weixin.qq.com/s/dovriJnogKp1rIg6YZG2dw)
      - [PyTorch 深度剖析：并行训练的 DP 和 DDP 分别在什么情况下使用及实例](https://blog.csdn.net/God_WeiYang/article/details/125289149)
      - [当代研究生应当掌握的并行训练方法（单机多卡）](https://mp.weixin.qq.com/s/HfE99ghT0Xv0F9pPCnNf6w)

### Dropout
  * R-Drop: [重新审视Dropout算法](https://mp.weixin.qq.com/s/uHkhrccuWYCLoHwIHMeSMA)

### EMA
  * [ema-pytorch](https://github.com/lucidrains/ema-pytorch): a simple way to keep track of an Exponential Moving Average (EMA) version of your pytorch model.

### Explaination
  * Survey
    - blog: [机器学习模型可解释性的详尽介绍](https://www.jiqizhixin.com/articles/2019-10-30-9)
  * SHAP(SHapley Additive exPlanations)
    - [code](https://github.com/slundberg/shap)
    - blog: [不再黑盒，机器学习解释利器：SHAP原理及实战](https://zhuanlan.zhihu.com/p/106320452)
  * LIME(Local Interpretable Model-agnostic Explanations)
    - [code](https://github.com/marcotcr/lime)
    - [paper](https://arxiv.org/abs/1602.04938)
    - blog: [能相信模型的预测吗？LIME：一种解释模型预测的方法](https://www.jiqizhixin.com/articles/2016-08-22-6)

### Feature Selection
  * Survey
    - blog: 
      - [特征选择：11 种特征选择策略总结！](https://mp.weixin.qq.com/s/BhCfPkJexi-DCLpwYwY6YA)
      - [Amazing-Feature-Engineering](https://github.com/ashishpatel26/Amazing-Feature-Engineering)（译文：[特征工程架构性好文](https://mp.weixin.qq.com/s/j1ofgcZ_Wcnn4ehbPnKM8w)）

### Finetune
  * [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/abs/1905.05583?context=cs.CL)

### Imbalanced Problem
  * Survey
    - blog:
      - [NLP实战|文本分类之样本不均衡处理及模型鲁棒性提升trick总结](https://mp.weixin.qq.com/s/mshop9mWDziqg9QwAyPD0Q)

### Label-Focused
  * Mix-up / [AutoMix](https://mp.weixin.qq.com/s/lOeamPcQ_I870j4xXk8EKg)
  * Label Smoothing (Soft Label)
  * [CAN](https://wmathor.com/index.php/archives/1589/)

### Lookahead

### Loss
  * Cross-Entropy Loss
    - [Things that confused me about cross-entropy](https://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/) by [Chris Said](https://chris-said.io/)

### Mask
  * Mask Ratio
    - blog: [丹琦女神又发新作，Mask 15%？我不](https://mp.weixin.qq.com/s/6bCwBy5_72Hl7-xr3hSZMA) （reserving my views :( ）

### Model-Fusion
  * Stacking
  * Ensemble

### Normalization
  * [归一化原来这么重要！深入浅出详解Transformer中的Normalization](https://mp.weixin.qq.com/s/n_twT43ZmQrkBKkAxOKa-Q)
  * [超细节的BatchNorm/BN/LayerNorm/LN知识点](https://mp.weixin.qq.com/s/rvs82W91jDPGyhcC_9PlLw)
  * [为什么Pre Norm的效果不如Post Norm？](https://mp.weixin.qq.com/s/kJnZpfYUIJRnLAUEuRQGsA)

### Optimizer
  * [pytorch-optimizer](https://github.com/kozistr/pytorch_optimizer): bunch of optimizer implementations in pytorch.
  * [Adan-Pytorch](https://github.com/lucidrains/Adan-pytorch): implementation of the ADAN (ADAptive Nesterov momentum algorithm) optimizer in pytorch.

### Parallelism
  * Survey
    - blog: [大规模模型训练tricks集锦](https://mp.weixin.qq.com/s/p99u10YOODDmZQPN0lc03w)

### Pre-Finetune
  * Survey
    - blog: [语言模型微调领域有哪些最新进展？一文详解最新趋势](https://mp.weixin.qq.com/s/XVZSAxaWM30t9rOeXYM03A) cited from [here](https://ruder.io/recent-advances-lm-fine-tuning/)

### Prior-Experience
  * Concatenate & Embedding
    - REINA([paper](https://arxiv.org/abs/2203.08773)/[code](https://github.com/microsoft/REINA)): [微软发现了一个超简单的NLP上分技巧，还发了ACL2022 ？？](https://mp.weixin.qq.com/s/koUKcfIozcl2zYeVzGj9wA)
  * Prompt
    - [哄一哄能让GPT-3准确率暴涨61%！谷歌&东京大学研究震惊四座](https://mp.weixin.qq.com/s/jDqaL6d2UCeY_8usXszkJg)
  * Unlearning
    - [模型会忘了你是谁吗？两篇Machine Unlearning顶会论文告诉你什么是模型遗忘](https://mp.weixin.qq.com/s/7J8sB5EBe_hsQqNYWyrDvg)

### Regularization
  * [如何用正则化防止模型过拟合？](https://mp.weixin.qq.com/s/5leQpMrdby5e9P6xCtE7mQ)

### Re-Parameter

### Softmax
  * Gumbel Softmax

### Warm-up
  * Linear Warm-up

### Other Trick-and-Tips
  * link: https://github.com/oukohou/trick-and-tips
  * [PyTorch 常用 Tricks 总结](https://mp.weixin.qq.com/s/E-WXOu5hmL1ynEJlHmA0WQ)
