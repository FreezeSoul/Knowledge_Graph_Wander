## **Tricks to Sharpen your Models**


### Adversarial Training
  * Survey
    - blog: [一文搞懂NLP中的对抗训练FGSM/FGM/PGD/FreeAT/YOPO/FreeLB/SMART](https://zhuanlan.zhihu.com/p/103593948)
  * FGM(Fast Gradient Method)
  * PGD(Projected Gradient Descent)
  * FreeLB (Free Large-Batch)

### Attention
  * DIN & DIEN
    - blog: [DIN+DIEN，机器学习唯一指定涨点技Attention](https://mp.weixin.qq.com/s/oRoy82I_8S7uvMToMouIeQ)

### BlockShuffle

### Cross-Validation
  * kFCV(k-Fold Cross-Validation)
  * LOO(Leave-One-Out Cross-Validation)

### Data Augmentation
  * Survey
    - blog: [一篇就够！数据增强方法综述](https://mp.weixin.qq.com/s/HPItY9xXJcOZWisBGOrkSw)
  * SDE
  * EDA
  * FlipDA: [为大模型定制的数据增强方法FlipDA，屠榜六大NLU 数据集！](https://mp.weixin.qq.com/s/EcC8naSuNrTNQf0Es32YHQ)

### EMA

### Explaination
  * Survey
    - blog: [机器学习模型可解释性的详尽介绍](https://www.jiqizhixin.com/articles/2019-10-30-9)
  * SHAP(SHapley Additive exPlanations)
    - [code](https://github.com/slundberg/shap)
    - blog: [不再黑盒，机器学习解释利器：SHAP原理及实战](https://zhuanlan.zhihu.com/p/106320452)
  * LIME(Local Interpretable Model-agnostic Explanations)
    - [code](https://github.com/marcotcr/lime)
    - [paper](https://arxiv.org/abs/1602.04938)
    - blog: [能相信模型的预测吗？LIME：一种解释模型预测的方法](https://www.jiqizhixin.com/articles/2016-08-22-6)

### Label-Focused
  * Mix-up
  * Label Smoothing
  * CAN

### Lookahead

### Mask
  * Mask Ratio
    - blog: [丹琦女神又发新作，Mask 15%？我不](https://mp.weixin.qq.com/s/6bCwBy5_72Hl7-xr3hSZMA) （reserving my views :( ）

### Model-Fusion
  * Stacking
  * Ensemble

### Normalization
  * [超细节的BatchNorm/BN/LayerNorm/LN知识点](https://mp.weixin.qq.com/s/rvs82W91jDPGyhcC_9PlLw)
  * [为什么Pre Norm的效果不如Post Norm？](https://mp.weixin.qq.com/s/kJnZpfYUIJRnLAUEuRQGsA)

### Pre-Finetune

### Warm-up
  * Linear Warm-up

### Other Trick-and-Tips
  * link: https://github.com/oukohou/trick-and-tips
