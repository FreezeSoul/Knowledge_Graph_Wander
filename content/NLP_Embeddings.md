## **Knowledge/Graph/Network Embedding**


### ELMo
  * [paper](http://export.arxiv.org/pdf/1812.10860): Samuel R. , Ellie P. , Edouard G. , Benjamin Van D. , Alex W. , Jan H. , Patrick X. , Raghavendra P. , R. T. M. , Roma P. , Najoung K. , Ian T. , Yinghui H. , Katherin Y. , Shuning J. , Berlin C. . (2018). Looking for ELMo's Friends: Sentence-Level Pretraining Beyond Language Modeling. 
  * code: [Origin by MLÂ²AT CILVR Report](https://github.com/nyu-mll/jiant), [Tutorial by Prashant Ranjan](https://github.com/PrashantRanjan09/Elmo-Tutorial), [keras by iliaschalkidis](https://github.com/iliaschalkidis/ELMo-keras), [Multi-Language-oriented ELMo by HIT-SCIR](https://github.com/HIT-SCIR/ELMoForManyLangs)

### Transformer
  * [paper](https://arxiv.org/pdf/1706.03762.pdf): Vaswani, A. , Shazeer, N. , Parmar, N. , Uszkoreit, J. , Jones, L. , & Gomez, A. N. , et al. (2017). Attention is all you need.
  * code: [tensorflow by Kyubyong](https://github.com/Kyubyong/transformer)

### Transformer-XL
  * [paper](https://arxiv.org/pdf/1901.02860.pdf): Dai, Z. , Yang, Z. , Yang, Y. , Carbonell, J. , Le, Q. V. , & Salakhutdinov, R. . (2019). Transformer-xl: attentive language models beyond a fixed-length context.
  * code: [tensorflow & pytorch by Zhilin Yang](https://github.com/kimiyoung/transformer-xl)

### GPT
  * [paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf): Alec R. , Karthik N. , Tim S. , & Ilya S. . (2018). Improving Language Understanding
by Generative Pre-Training
  * code: [tensorflow by OpenAI](https://github.com/openai/finetune-transformer-lm)

### BERT
  * [paper](https://arxiv.org/pdf/1810.04805.pdf): Devlin, J. , Chang, M. W. , Lee, K. , & Toutanova, K. . (2018). Bert: pre-training of deep bidirectional transformers for language understanding.
  * code: [tensorflow by google-research](https://github.com/google-research/bert), [pytorch by Hugging Face
Report abuse](https://github.com/huggingface/pytorch-pretrained-BERT)

### GPT-2
  * [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf): Alec R. , Jeffrey W. , Rewon C. , David L. , Dario A. , Ilya S. . (2019). Language Models are Unsupervised Multitask Learners.
  * code: [tensorflow by OpenAI](https://github.com/openai/gpt-2)

### MASS
  * [paper](https://arxiv.org/pdf/1905.02450.pdf): Kaitao S. , Xu T. , Tao Q. , Jianfeng L. , Tie-Y. L. . (2019). MASS: Masked Sequence to Sequence Pre-training for Language Generation.

